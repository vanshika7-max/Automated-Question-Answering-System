{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q/A System building-Team1 - Capstone Project1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDU9gXUwgpbdSZULdx4XWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanshika7-max/Automated-Question-Answering-System/blob/main/Q_A_System_building_Team1_Capstone_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GluC4cH31uz4"
      },
      "source": [
        "# **Problem Statement**\n",
        "We will solve the above-mentioned challenge by applying deep learning algorithms to textual data.\n",
        "The solution to this problem can be obtained through Extractive Question Answering wherein we can\n",
        "extract an answer from a text given the question.\n",
        "###1.2.1 Topic Modelling\n",
        "This is a theme extraction task on a collection of Data Science specific documents which can be done\n",
        "via Latent Dirichlet Allocation (LDA). The topic model should identify the important themes of a\n",
        "document and list down the top-N constituent words of the themes/topics.\n",
        "###1.2.2 Extractive Question Answering\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. The\n",
        "text would essentially be the group of documents that have the highest concentration of the topic\n",
        "closest to the asked question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC_M4lSR2BVh"
      },
      "source": [
        "## **1.2.2.1 Head-start References**\n",
        "❖ https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "<br>❖ https://pyldavis.readthedocs.io/en/latest/readme.html\n",
        "<br>❖ https://huggingface.co/transformers/usage.html#extractive-question-answering\n",
        "###NOTE - The solution should not be limited to the above references; students are encouraged to read relevant research papers.\n",
        "### 1.3 Scope of project\n",
        "<br>A. The topic model should be able to identify/extract important topics.\n",
        "<br>B. The topic model would be built on the corpus of Data Science documents.\n",
        "<br>C. The topic model should yield the most relevant and stable topics measured through the\n",
        "perplexity score.\n",
        "<br>D. Once the relevant documents have been retrieved, the extractive question answering\n",
        "<br>model would generate the answer for the question.\n",
        "<br>E. The entire dual-model pipeline would be deployed in AWS/GCP/Azure\n",
        "<br>F. The dual-model pipeline must be accessible via a web application(Streamlit) for demo\n",
        "purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3Bsl7-NOxc"
      },
      "source": [
        "# **Part-1 - Making DataFrame in CSV Form**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IKTWHUcEzts"
      },
      "source": [
        "## **Importing Importent Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuP5ii7yDIeQ"
      },
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFshL_8eFMKX",
        "outputId": "d8bdabd7-cba4-4202-81af-cb247cd97a0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZe6wXqnbpTd"
      },
      "source": [
        "###################################################################################\n",
        "## file      information_retrieval_system.py\n",
        "#  brief     The information_retrieval_system.py is a basic information retrieval system  \n",
        "#             implemented using Python, NLTK and GenSIM.\n",
        "###################################################################################\n",
        "\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from gensim import corpora, models, similarities\n",
        "from operator import itemgetter\n",
        "import abc\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGxvzdh6bylL"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoNylarpbqau",
        "outputId": "29809053-c798-484b-92e8-ab0c80765d8a"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utqwnm0cP5ja"
      },
      "source": [
        "\n",
        "\n",
        "###################################################################################\n",
        "## class   InformationRetrievalSystem\n",
        "#  brief   This class represents the InformationRetrievalSystem, i.e., basic methods \n",
        "#           used to preprocess and rank documents according to user queries.\n",
        "###################################################################################\n",
        "\n",
        "class IRSystem(object):\n",
        "    \n",
        "    #################################################################################\n",
        "    ## brief   Constructor\n",
        "    #  details This method initializes the class with the parameters introduced by \n",
        "    #           the user and execute the query. \n",
        "    #################################################################################    \n",
        "    def __init__(self, corpus,queries):\n",
        "        __metaclass__ = abc.ABCMeta\n",
        "        self.corpus = corpus\n",
        "        self.queries= queries\n",
        "\n",
        "    #################################################################################\n",
        "    ## brief   preprocess_document\n",
        "    #  details This method return the taxonomy of keywords for the given document.\n",
        "    #  param   doc The document to be preprocessed\n",
        "    #################################################################################    \n",
        "    def preprocess_document(self,doc):\n",
        "        stopset = set(stopwords.words('english'))\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = wordpunct_tokenize(doc) # split text on whitespace and punctuation\n",
        "        clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
        "        final = [stemmer.stem(word) for word in clean]\n",
        "        return final\n",
        "\n",
        "\n",
        "    #################################################################################\n",
        "    ## brief   create_dictionary\n",
        "    #  details This method creates a dictionary based on the taxonomy of keywords for each document.\n",
        "    #  param   docs The documents to be preprocessed\n",
        "    #################################################################################\n",
        "\n",
        "    def create_dictionary(self,docs):\n",
        "        pdocs = [self.preprocess_document(doc) for doc in docs]\n",
        "        dictionary = corpora.Dictionary(pdocs)\n",
        "        dictionary.save('vsm.dict')\n",
        "        return dictionary,pdocs\n",
        "\n",
        "    #################################################################################\n",
        "    ## brief   get_keyword_to_id_mapping\n",
        "    #  details This method prints the tokens id (word counts) for the given dictionary.\n",
        "    #  param   dictionary The dictionary with the documents keywords.\n",
        "    #################################################################################    \n",
        "    def get_keyword_to_id_mapping(self,dictionary):\n",
        "        print (dictionary.token2id)\n",
        "\n",
        "    #################################################################################\n",
        "    ## brief   docs2bows\n",
        "    #  details This method converts document (a list of words) into the bag-of-words\n",
        "    #  format = list of (token_id, token_count) 2-tuples.\n",
        "    #  param   corpus Set of documents to be processed.\n",
        "    #  param   dictionary The dictionary with the documents keywords.\n",
        "    #################################################################################    \n",
        "    def docs2bows(self,corpus, dictionary, pdocs):\n",
        "        vectors = [dictionary.doc2bow(doc) for doc in pdocs]\n",
        "        corpora.MmCorpus.serialize('vsm_docs.mm', vectors) # Save the corpus in the Matrix Market format\n",
        "        return vectors\n",
        "\n",
        "\n",
        "\n",
        "    def ranking_function(self,corpus, q, mode):\n",
        "        model, dictionary = self.create_documents_view(corpus, mode)\n",
        "        loaded_corpus = corpora.MmCorpus('vsm_docs.mm')\n",
        "        index = similarities.MatrixSimilarity(loaded_corpus, num_features=len(dictionary))\n",
        "        vq=self.create_query_view(q,dictionary)\n",
        "        self.query_weight = model[vq]\n",
        "        sim = index[self.query_weight]\n",
        "        ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
        "        for doc, score in ranking:\n",
        "            print (\"[ Score = \" + \"%.3f\" % round(score, 3) + \"] \" + corpus[doc]);\n",
        "      \n",
        "   \n",
        "    #################################################################################  \n",
        "    def create_query_view(self,query,dictionary):\n",
        "        pq = self.preprocess_document(query)\n",
        "        vq = dictionary.doc2bow(pq)\n",
        "        return vq\n",
        "  \n",
        "\n",
        "    #################################################################################\n",
        "    ## brief   create_documents_view\n",
        "    #  details This method preprocess the documents written in NL to build the documents view\n",
        "    #  param   corpus Set of documents to be processed.\n",
        "    #################################################################################  \n",
        "    def create_documents_view(self,corpus, ir_mode):\n",
        "        dictionary,pdocs = self.create_dictionary(corpus)\n",
        "        bow = self.docs2bows(corpus, dictionary,pdocs)     \n",
        "        loaded_corpus = corpora.MmCorpus('vsm_docs.mm') # Recover the corpus\n",
        "        model = models.TfidfModel(loaded_corpus) # TF IDF model\n",
        "        return model, dictionary\n",
        "\n",
        "    def query_launcher(self,corpus, queries, mode):\n",
        "      self.ranking_function(corpus,queries,mode)\n",
        "      return\n",
        "\n",
        "#class IR_tf(IRSystem):\n",
        "\n",
        "#  def __init__(self,corpus,queries):\n",
        "#         IRSystem.__init__(self,corpus,queries)\n",
        "#         print(\"\\n--------------------------Executing TF information retrieval model--------------------------\\n\")\n",
        "#         self.ranking_query=dict()\n",
        "#         self.query_launcher(corpus,queries,0)\n",
        "\n",
        "\n",
        "class IR_tf_idf(IRSystem):\n",
        "\n",
        "    def __init__(self,corpus,queries):\n",
        "        IRSystem.__init__(self,corpus,queries)\n",
        "        print(\"\\n--------------------------Executing TF IDF information retrieval model--------------------------\\n\")\n",
        "        self.ranking_query=dict()\n",
        "        self.query_launcher(corpus,queries,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "6EVCefa4bEWX",
        "outputId": "e02bca91-77e3-4dfd-f069-9ab83c03e2e4"
      },
      "source": [
        "tf_idf = IR_tf_idf(corpus_text,query_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------Executing TF IDF information retrieval model--------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7356b61761b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIR_tf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, queries)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--------------------------Executing TF IDF information retrieval model--------------------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36mquery_launcher\u001b[0;34m(self, corpus, queries, mode)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquery_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36mranking_function\u001b[0;34m(self, corpus, q, mode)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mranking_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_documents_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mloaded_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vsm_docs.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36mcreate_documents_view\u001b[0;34m(self, corpus, ir_mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m#################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_documents_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs2bows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mloaded_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vsm_docs.mm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Recover the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36mcreate_dictionary\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mpdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vsm.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mpdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vsm.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f66fdfc8bdcd>\u001b[0m in \u001b[0;36mpreprocess_document\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstopset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# split text on whitespace and punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrGJX4RBZtgJ",
        "outputId": "53e75ca0-eccb-4c63-fe92-c4f0161c972c"
      },
      "source": [
        "# Import modules needed for this project\n",
        "!pip install pdfplumber\n",
        "import pdfplumber"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/4d9768e9ed204c68bd5813a2a112d3d6af4912f0785d47080b5067cdce64/pdfplumber-0.5.27.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20200517\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/c0/ef1c8758bbd86edb10b5443700aac97d0ba27a9ca2e7696db8cd1fdbd5a8/pdfminer.six-20200517-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Collecting Wand\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/f6/05f043c099639b9017b7244791048a4d146dfea45b41a199aed373246d50/Wand-0.6.6-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.3.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Building wheels for collected packages: pdfplumber\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.27-cp37-none-any.whl size=32071 sha256=77870f4ed95a9f398ec4dc2d60dfa420b631f560ab65f1e0582e33bcb9f4a69a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/74/fc/f7b3a1a0732209027fb48a5f4392fc40d79970b11c2ba49e71\n",
            "Successfully built pdfplumber\n",
            "Installing collected packages: pycryptodome, pdfminer.six, Wand, pdfplumber\n",
            "Successfully installed Wand-0.6.6 pdfminer.six-20200517 pdfplumber-0.5.27 pycryptodome-3.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63rtPEuu8s3r",
        "outputId": "e87f7d0c-4dca-417e-c10a-a92de95147ba"
      },
      "source": [
        "#################################################################################\n",
        "#  brief   preprocess_input\n",
        "#  details This method reads user input and transform it into a list\n",
        "#  param   user_input The input given by the user\n",
        "#################################################################################  \n",
        "def preprocess_userinput(infile):\n",
        "    pgList=[]\n",
        "    with pdfplumber.open(infile) as pdf:\n",
        "      totalpages = len (pdf.pages)\n",
        "      for i in range(0,totalpages):\n",
        "        page = pdf.pages[i]\n",
        "        row = page.extract_text().split('\\n')\n",
        "        pgList.append(row)\n",
        "      return pgList\n",
        "\n",
        "#################################################################################\n",
        "## brief   create_ir_system\n",
        "#  details This method creates an information retrieval system with the model \n",
        "#           chosen by the user\n",
        "#  param   irmodel_choice The id of the information retrieval model chosen by the user\n",
        "#################################################################################  \n",
        "def create_ir_system(irmodel_choice,corpus):\n",
        "  return ir_system.IR_tf_idf(corpus)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':     \n",
        "      corpus_input = input(\"Write a text or enter the corpus path:\\n\") \n",
        "      corpus_text=preprocess_userinput(corpus_input)\n",
        "\n",
        "      query_text = input(\"Query Enter By User:\\n\")\n",
        "      # ir = execute_IRsystem_prompt(corpus_text)\n",
        "      # rocchio = execute_Rocchio_prompt(query_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a text or enter the corpus path:\n",
            "/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 8/Q A System Building/Automated Q_A PDFs/Applied Data Science.pdf\n",
            "Query Enter By User:\n",
            "What is Data Science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndywhDTOA_nF"
      },
      "source": [
        "corpus_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHtoVl-nkSYC"
      },
      "source": [
        "import pickle\n",
        "with open('tfidf.pkl', 'wb') as p:\n",
        "    pickle.dump(tfidf,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zUynfXKZCwBH",
        "outputId": "9572315e-e394-45eb-853d-e0afa765656a"
      },
      "source": [
        "query_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What is Data Science?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DAkTGtj3YQU"
      },
      "source": [
        "IR_tf_idf."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}